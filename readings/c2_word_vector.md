- Example: Textual Similarity
  - Given 2 sentences, we want to determine 2 sentences are similar or not 
  <img width="1065" alt="Screenshot 2022-04-20 at 19 11 13" src="https://user-images.githubusercontent.com/64508435/164218294-72d8da77-2d9e-4bd3-81c9-571a2838b19c.png">
  - Problem: Similar meanings but different words
- [1. Corpus Pre-processing](#1-corpus-pre-processing)


# 1. Corpus Pre-processing
Data preprocessing steps 
- tokenization
‚Äì case-folding (lower-case
‚Äì Normalization
‚Äì Stopwords removal
## 1.1. Regular Expression
- Definition:
  - Search *pattern* used to match character combinations in a string
  - Pattern: a sequence of characters
- Application:
  - Search and replace
  - Validate texts to ensure given patterns (for example: password validation)
  - <img width="607" alt="Screenshot 2022-04-20 at 19 16 34" src="https://user-images.githubusercontent.com/64508435/164219135-c46cd292-0357-43fa-b905-18573e7e2863.png">

- Fixed pattern: **Exact** match
- Meta characters: **Relaxed** match a type of characters
<img width="405" alt="Screenshot 2022-04-20 at 19 16 18" src="https://user-images.githubusercontent.com/64508435/164219098-30b155b6-7cb5-4c81-b40b-527571855b43.png">

- Character class:<img width="675" alt="Screenshot 2022-04-20 at 19 20 44" src="https://user-images.githubusercontent.com/64508435/164219763-c610452a-937b-4213-9879-a1079485f99a.png">

- Repetition: repeated match<img width="514" alt="Screenshot 2022-04-20 at 19 18 14" src="https://user-images.githubusercontent.com/64508435/164219352-9a354e34-c9a0-4563-8878-cd453f4c7f63.png">

- Example: Regular Expression
- <img width="607" alt="Screenshot 2022-04-20 at 19 19 55" src="https://user-images.githubusercontent.com/64508435/164219626-897d01b9-fd96-4544-ac78-071a4df8ebbf.png">
- Group: 
- Enclosing with ‚Äú( ... )‚Äù
- Match the whole pattern, while capture each group individually 
- Groups can be nested ‚Äú( ... (... (...))...)‚Äù

<img width="545" alt="Screenshot 2022-04-20 at 19 22 05" src="https://user-images.githubusercontent.com/64508435/164219973-e1c41737-0121-41d8-98ae-3b1f5776a708.png">

- Backreference: Find repeated group
<img width="601" alt="Screenshot 2022-04-20 at 19 22 36" src="https://user-images.githubusercontent.com/64508435/164220044-16e3df75-411a-4367-af5a-d27542c6f41b.png">
  - `\2`: back-reference the second group
  - Matched words: `m-o-m`, `t-e-s-t`

- Group assertions: Capture ahead or behind the match
<img width="601" alt="Screenshot 2022-04-20 at 19 25 50" src="https://user-images.githubusercontent.com/64508435/164220488-9906e317-4425-46e6-bda2-693f6aa0eff9.png">

## 1.2. Tokenization
- **Tokenization**: Split a piece of text into meaningful units (i.e., tokens)
- **Token**: word, sub-word (like morphemes), numbers, or punctuation, etc.

- Important steps for word representation
  ‚Äì Especially for the languages without word spacing (like Chinese)
- Method 1: Regular Expression
  - Lightweight, efficient in practice
- Method 2: Using packages, e.g., spaCy
  - Limitation: languages with whitespaces separating words   
<img width="699" alt="Screenshot 2022-04-20 at 19 32 54" src="https://user-images.githubusercontent.com/64508435/164221552-efeb7322-e867-405a-b2f3-ee22d3da23ef.png">

### 1.2.2. Tokenization for languages without separate whitespaces
- Tokenization for languages without separate whitespaces ?
- Intuitive solution
  ‚Äì First collect a dictionary of words 
  - Then conduct Maximum Matching
- Limitation: perform worse on languages with whitespaces
<img width="713" alt="Screenshot 2022-04-20 at 19 34 30" src="https://user-images.githubusercontent.com/64508435/164221795-ee4ce6ba-6ab1-4e2f-a56a-eb07be464408.png">

<img width="480" alt="Screenshot 2022-04-20 at 19 35 00" src="https://user-images.githubusercontent.com/64508435/164221871-4d015b5f-6d13-4bc7-8bd0-8da08e1a445f.png">

### 1.2.3. Tokenization using Sub-word
- Tokenization Sub-word: Split texts into frequent subwords
- Current state-of-the-art:
  - **Byte-Pair Encoding (BPE)**: GPT, Roberta, XLM 
  - **WordPiece**: BERT, DistilBERT, Electra
#### 1.2.3.1. Tokenization: BPE
- Two steps:
  - Token learning: learn a vocabulary from corpus 
  - Tokenizer: split text according to the vocabulary

<img width="625" alt="Screenshot 2022-04-20 at 19 40 29" src="https://user-images.githubusercontent.com/64508435/164222677-05cbda66-0921-407e-8c25-b9949359abd1.png">
 - Step 1: Token Learning
  - `k`: how many sub-words that we want 
  - Initialize the vocab with single character
  - Find most frequent two consecutive.
  - By Product: Merge Rules
<img width="643" alt="Screenshot 2022-04-20 at 19 41 52" src="https://user-images.githubusercontent.com/64508435/164222889-04c3a6ad-6beb-4d0c-a9b6-bb652de93e7f.png">
    - Find most frequent two consecutive. In this example is `es`
    <img width="681" alt="Screenshot 2022-04-20 at 19 43 48" src="https://user-images.githubusercontent.com/64508435/164223175-f0d5e6cc-2cb5-4c83-a88e-c032788b400f.png">
<img width="681" alt="Screenshot 2022-04-20 at 19 44 06" src="https://user-images.githubusercontent.com/64508435/164223217-521eac6d-0397-4dc6-8868-350aeca3cdb8.png">

- Step 2: Tokenier
  - Base on the **merge rules**, we will build the tokenizer. 
<img width="594" alt="Screenshot 2022-04-20 at 19 45 38" src="https://user-images.githubusercontent.com/64508435/164223468-7d45b76e-21a1-4cf9-a9ec-92ff2b665d6c.png">

#### 1.2.3.2. Tokenization: WordPiece
- Mostly, similar to BPE
- WordPiece does not choose the most frequent pair, but the one that **maximizes the likelihood of the training data once added to the vocabulary**.
  - evaluates what it loses by merging two symbols to ensure it‚Äôs worth it.
  
<img width="685" alt="Screenshot 2022-04-20 at 19 54 55" src="https://user-images.githubusercontent.com/64508435/164224944-35b95677-0c21-446b-977b-49c296ab1d8f.png">

## 1.3. Normalization
- Convert the meaningful unit into a canonical (one) form
  - Lemma, Stem
<img width="664" alt="Screenshot 2022-04-20 at 19 57 18" src="https://user-images.githubusercontent.com/64508435/164225282-1f9ca9c6-3a4a-4eba-87a6-01f7fe399187.png">

- Not always necessary 
  - helpful in Information Retrieval
  - may bring noise in Named Entity Recognition

<img width="664" alt="Screenshot 2022-04-20 at 19 58 14" src="https://user-images.githubusercontent.com/64508435/164225460-6ed72aff-603a-4846-8251-1f4e0bcfba09.png">

### 1.3.1. Normalization - Stemming
- Idea: Reduce words to their stem (stem: meaningful unit in a word)
‚Ä¢ Method: removing affixes based on rules ‚Äì Singular vs plural
‚Äì verb tenses
‚Äì comparative/superlative adjective
‚Ä¢ Pro: fast
‚Ä¢ Con: Stem word may not be a word

<img width="312" alt="Screenshot 2022-04-20 at 19 59 37" src="https://user-images.githubusercontent.com/64508435/164225691-e96bd3a1-453f-48ef-a19a-bd29af6de2ca.png">

- Porter Stemmer:  most common stemmer for English
  - Series of rules that run in a cascade:
    - output of each pass is fed into the next pass 
    - stemming stops if a pass makes no changes

<img width="487" alt="Screenshot 2022-04-20 at 20 01 22" src="https://user-images.githubusercontent.com/64508435/164226008-379102b3-29ab-40cf-87a5-5acbe809fef6.png">

### 1.3.2. Normalization - Lemmatization
- Idea: convert words to the base form
- Lemma (Base form) will depend on the POS (Part of Speed) tage of a word like Lemma (n), Lemma (v)

- Pro:
  ‚Äì Lemma words are proper words
  - Can normalize irregular forms
- Con:
  ‚Äì require lexicon + rules ‚Äì require POS tags
  - slower than stemming as need to identify POS tags
<img width="542" alt="Screenshot 2022-04-20 at 20 02 46" src="https://user-images.githubusercontent.com/64508435/164226217-682e3a2f-43ab-40c3-a1fd-d03b00c6eda3.png">

#### Final words for Normalization:
- Canonical form also effects tokenization
  - Separate out clitics (e.g., doesn‚Äôt -> does n‚Äôt, John‚Äôs -> John ‚Äòs) ‚Äì Keep hyphenated words together
  - Separate out all punctuation symbols
- Other common normalization steps
  - Removal of stop words (e.g, a, an, the)
  - Removal of non-standard tokens (e.g., url, emojis)


# 2. Word Vector
- Motivation: Find the similar words
## 2.1. Sparse Word Embeddings
### 2.1.1. One-hot Encoding 
<img width="677" alt="Screenshot 2022-04-20 at 20 11 07" src="https://user-images.githubusercontent.com/64508435/164227554-089f9e07-52d5-46dd-a2e9-081e93b976db.png">

### 2.1.2. Word Net
- WordNet is a lexical knowledge base 
  - mostly **manually** curated;
  ‚Äì gather similar words into synsets
  - `synsets` are connected via various relations

<img width="689" alt="Screenshot 2022-04-20 at 20 14 44" src="https://user-images.githubusercontent.com/64508435/164228136-0ee7934c-5874-4c64-bc9b-cd023d294308.png">

- WordNet based Word similarity (e.g., Wu & Palmer Similarity) 
  - Topological similarity between synsets
  - Consider the depth of Least Common Subsumer (LCS)
![IMG_AE8EC93AAAA2-1](https://user-images.githubusercontent.com/64508435/164228835-8c53aa27-1f27-466c-a73a-547d66f55dd5.jpeg)
![IMG_22263D3C34B7-1](https://user-images.githubusercontent.com/64508435/164228928-35e73c1b-ca33-46b5-a9fe-56a8bea99b76.jpeg)
<img width="700" alt="Screenshot 2022-04-20 at 20 20 58" src="https://user-images.githubusercontent.com/64508435/164229159-210c2aee-b016-4fff-bbcc-d5efc607cb74.png">

- Limitation of WordNet similarity
  - Granularity was not fine enough
    - e.g. good vs. effective, charter vs. lease
  - Difficult to keep up-to-date
    - e.g. cloud computing, badass, COVID
  - Human labor intensive

### 2.1.3. Co-Occurrence Vectors
‚Äì A vector has a length of vocabulary size (number of unique words)
‚Äì Each element denotes how often two words occurs with each other (within a predefined window).

<img width="716" alt="Screenshot 2022-04-20 at 20 23 50" src="https://user-images.githubusercontent.com/64508435/164229612-2cdddbd8-0d1f-4ffe-bd71-955205abe4e3.png">
- In this example: we count the occurence in the entire sentence.
<img width="716" alt="Screenshot 2022-04-20 at 20 25 58" src="https://user-images.githubusercontent.com/64508435/164229926-21c88344-35af-4796-b2da-9b252f49ecf2.png">

- Consideration #1: Relative importance 
  - e.g., is ‚Äúcrowd‚Äù twice important than ‚Äúwork‚Äù?
- Consideration #2: Discriminative
  - e.g., some words are just very frequent

Solution: from raw count to **Pointwise Mutual Information** (PMI)

#### Pointwise Mutual Information (PMI)
- `p(w1)`, `p(w2)`: probability of unigram, or probability of the word w1 appears in the corpus.
- `pmi(w1, w2)`: whether two words w1, and w2 appear together more freq than if they appear alone. 
- <img width="428" alt="Screenshot 2022-04-20 at 20 50 32" src="https://user-images.githubusercontent.com/64508435/164234315-27c8bfee-09a7-4371-b606-2563e4b0b904.png">

##### Information Theory
- When x occurs, how much information it brings, aka, surprisal of x.
- Inversely proportional to probability
<img width="405" alt="Screenshot 2022-04-20 at 20 55 51" src="https://user-images.githubusercontent.com/64508435/164235290-23079be5-4277-4634-8631-05104cda4315.png">
- Example 1:
<img width="640" alt="Screenshot 2022-04-20 at 20 56 32" src="https://user-images.githubusercontent.com/64508435/164235424-48bc4bfb-f27c-494e-a8df-389afcd737a1.png">
- Example 2: danger brings more information than "the", "an", "and"

<img width="640" alt="Screenshot 2022-04-20 at 20 57 36" src="https://user-images.githubusercontent.com/64508435/164235611-329fde8d-8730-43bd-9838-c05d6eed48b5.png">

##### Entropy
- Expectation of information contents
- More uncertainty, higher entropy
- High Entropy, the system will be un-stable as there is a lot of uncertainty.

<img width="395" alt="Screenshot 2022-04-20 at 20 59 29" src="https://user-images.githubusercontent.com/64508435/164235916-7961bbfa-1059-4581-897e-0634554ab676.png">


##### Mutual Information
- Mutual Information: Entropy betweeen two systems
- If X & Y independent, then I(X,Y) = 0
- I see the word "movie", it will have me to guess the next words, for example "star", "actor"
- Mutual information is the average level of PMI.
<img width="678" alt="Screenshot 2022-04-20 at 21 01 36" src="https://user-images.githubusercontent.com/64508435/164236277-36f5e654-dc74-45cd-a4bd-b5f085d23984.png">

### Converting Count-based Matrix to PMI Matrix

![IMG_AC5E0FDC50AE-1](https://user-images.githubusercontent.com/64508435/164237310-850046c4-6988-4960-b67d-9377b9a9ab48.jpeg)

#### Limitation of 
‚Äì Vector size is too large 1 x |ùëâ| , (|ùëâ| is typically 20k~200k)
‚Äì Vector is too sparse (most dimensions are zero value) 
  - Various improvements, e.g., Add-1 smoothing


## 2.2. Dense Word Embeddings
- A dense vector (a.k.a.  word embeddings) that maps the similarity between words to the distance in vector space.

<img width="695" alt="Screenshot 2022-04-20 at 21 10 18" src="https://user-images.githubusercontent.com/64508435/164237724-78f880de-c540-42be-8ec6-b4b52ea6cd19.png">

### 2.2.1. Why Dense Word Vectors
- Advantage
  - Efficient Computation
  - Less parameters
- In practice
  - 100-1000 dimensions (vs 20k-200k in sparse vectors)
  - almost no dimension is zero-value

<img width="695" alt="Screenshot 2022-04-20 at 21 11 51" src="https://user-images.githubusercontent.com/64508435/164238024-a16ee46d-6016-4db1-8096-8dac0a81a137.png">

# 3. Dense Word Embedding Method
## 3.1. PMI Matrix Factorization
- use Singular Value Decomposition (SVD) to decompose the PMI Matrix into U, S, V

<img width="695" alt="Screenshot 2022-04-20 at 21 14 41" src="https://user-images.githubusercontent.com/64508435/164238566-25d1e64b-cd7c-4d59-8ce6-56a458313932.png">

![IMG_F5D77784CB01-1](https://user-images.githubusercontent.com/64508435/164239820-0cd4cd98-1e87-4abb-a388-527e643c93bf.jpeg)

## 3.2. Word2Vec
- Basic idea:
  - given a corpus, we slide a window to obtain many spans of words to capture the context
  - predict if two words co-occur in the same window (Binary classification)
![IMG_C716C034C721-1](https://user-images.githubusercontent.com/64508435/164240768-45e41469-f8e8-48d3-8080-8bf50a1a0699.jpeg)
- Two variants
  - CBOW: Continuous Bag of Words (for Large Corpus)
  - Skip-gram (For Small Corpus)

### 3.2.1. CBOW 
- Continuous Bag-of-Words (CBOW): predict a word from the context
-
<img width="695" alt="Screenshot 2022-04-20 at 21 28 15" src="https://user-images.githubusercontent.com/64508435/164241123-ee763f20-ee3c-4f3a-9ae3-d8596851e462.png">

### 3.2.2. Skip-gram
- Skip-gram: Predict context from a word
<img width="695" alt="Screenshot 2022-04-20 at 21 29 00" src="https://user-images.githubusercontent.com/64508435/164241276-a84d6a6f-5a5d-4998-bb0d-8ddb2a368b68.png">

<img width="703" alt="Screenshot 2022-04-20 at 21 55 44" src="https://user-images.githubusercontent.com/64508435/164246721-c2bc0ad9-9fad-4106-9d2f-f386ff10695e.png">

### 3.2.3. Learning Objective
- Two-level goals
  - Optimize current word vector close to its context words
  - words with similar contexts will be close (or similar embedding)
<img width="703" alt="Screenshot 2022-04-20 at 21 57 45" src="https://user-images.githubusercontent.com/64508435/164247178-94167267-dd32-4d9b-94b2-69abce40bce4.png">

### 3.2.4. Negative Sampling
- To avoid the calculation over the entire vocabulary


## 3.3. Glove
- Combine both *local* and *global* features
  ‚Äì **Local features**, e.g., context span in a window of Word2Vec
  ‚Äì **Global features**, e.g., PMI matrix over the entire corpus of SVD

<img width="703" alt="Screenshot 2022-04-20 at 22 09 26" src="https://user-images.githubusercontent.com/64508435/164249566-0e76a0c3-5f4d-4b09-a406-798cb607c38a.png">

- Step1: Build Co-occurrence Probabilities Matrix to capture word similarity
<img width="703" alt="Screenshot 2022-04-20 at 22 10 48" src="https://user-images.githubusercontent.com/64508435/164249854-6fe5356b-7254-4238-acab-cb3c5e95bebb.png">
- Bayes Theorem: to compute P(X|Y)

<img width="716" alt="Screenshot 2022-04-20 at 22 11 21" src="https://user-images.githubusercontent.com/64508435/164249960-32ab0e21-23fb-443c-9a18-9380f9903fdd.png">
- Wi = ice, Wj=Steam, Wk = x (random)
- There are 4 cases for the ratio to capture the global feature
  - Large
  - Small
  - ~1 
  - ~1

- Step2:
  - Learn word vectors to fit the ratios
  - suppose we have word vectors that meet:
  <img width="716" alt="Screenshot 2022-04-20 at 22 14 50" src="https://user-images.githubusercontent.com/64508435/164250733-aa5622e9-02b7-4456-b168-6d6903214bf1.png">

![IMG_75107973B630-1](https://user-images.githubusercontent.com/64508435/164251341-4b2856c2-e2ce-48be-a125-ec680b2999ff.jpeg)
![IMG_152D989247B3-1](https://user-images.githubusercontent.com/64508435/164251993-a8f0af96-ec16-45b4-bef1-627392bb099d.jpeg)
![IMG_2630FEF38F14-1](https://user-images.githubusercontent.com/64508435/164252159-039ae382-6cca-42b3-8dfd-fef004019cf4.jpeg)

# 4. Evaluation of Word Embedding
- Word similarity: check if two words are similar with each other using Cosine Similarity
- Word Analogy: Vector differences reflect semantic relationship

<img width="716" alt="Screenshot 2022-04-20 at 22 22 58" src="https://user-images.githubusercontent.com/64508435/164252525-d7759aab-2df9-47a6-9178-bc0c51f24b3d.png">

# 5. Limitation of Word Embedding
- How to represent rare or unseen words? 
  - e.g., words not included in the corpus
- How to represent cross-language, sentiment, negation, or other semantics?
  - e.g., ‚Äúscary‚Äù is similar with ‚Äúfunny‚Äù, is this right? ‚Äì e.g., ‚Äúnot good‚Äù vs ‚Äúgood‚Äù vs ‚Äúbad‚Äù
- How to represent different senses of the same word? ‚Äì e.g., ‚Äúbass‚Äù, ‚Äúbank‚Äù, ‚Äúapple‚Äù
- How to represent phrases?
  - e.g., ‚ÄúNew York‚Äù not new and york, ‚Äúice cream‚Äù, ‚Äúhot dog‚Äù.

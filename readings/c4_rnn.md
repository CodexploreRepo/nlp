# Recurrent Neural Networks


# Table of contents
- [Table of contents](#table-of-contents)


# Test
-  Bag-of-words(BoW) models: Naive Bayes, Word2Vec Embedding  
  - Problem: we are ignore the order of words, tokens when counting
  
<p align="center"><img width="600" alt="Screenshot 2022-04-20 at 19 25 50" src="https://user-images.githubusercontent.com/64508435/166672704-500ad9d9-ff40-4379-b994-465c51859939.png"></p>

- Language Model: 
- Markov Assumption: given the current state, the future state is independent with past states. 
  - Given a couple of words, the next words are independent with the pass words
  - Probabilities depend on only the last k words (i.e. the words nearby), instead of long dependency.


